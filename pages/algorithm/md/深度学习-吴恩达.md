---
layout: page
title: 吴恩达 深度学习 笔记
date:   2019-10-29
---

## 

### 第一周 深度学习概率

简要介绍深度学习发展和课程内容

### 第二周 神经网络基础

logistic回归

m个输入样本x，x为n维列向量

m个输出y，y为二值

数据归一化用sigmoid函数，作为激活函数。提到了Relu激活函数，此函数由于其曲线特性，训练速度会很快。

假设我们认为输入输出满足如下关系

$ y=\sigma(w^T x+b)$

损失函数

$L( \hat{y},y)=-ylog(\hat{y}) + (1-y)log(1-\hat{y})$

为什么不用$L( \hat{y},y)= \frac{1}{2} (\hat{y}^2 - y^2)$

是为了使求解问题变成凸优化问题（没有理解,课程里暂时未解释）

最终的Cost函数就是所有样本损失函数的均值

单次迭代有

$ Z=w^T + b $
$ A = \sigma(Z) $
$ dZ = A - Y $
$ dw = \frac{1}{m} X dZ^T$
$ db = \frac{1}{m} dZ^T$
$ w = w - \alpha dw $
$ b = b - \alpha db $

这也是为什么就算多个样本也要多次迭代的原因。


个人理解：

和之前理解的训练不一样，最初以为是每个样本单次输入迭代到最后一个样本训练结束。而把所有样本作为单次迭代的训练方法，最后结果应该是接近最优解的。单个样本依次训练可能无法保证。

但是直观上来说，生物体在学习的过程中应该做不到每次都把所有的数据都训练一边。记忆有限，而且在网络训练过程中，早期输入的样本影响会很大，毕竟启蒙教育很重要……

超限学习ELM应该是一次把所有样本代入，直接通过最小二乘法计算权重。对于大量样本的训练可能不适合，这也是为何ELM是一个浅层神经网络。